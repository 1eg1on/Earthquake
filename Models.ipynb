{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ha-ha classics\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import os \n",
    "\n",
    "# sklearn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "# 2nd category\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier as lgbm\n",
    "\n",
    "# 3rd category\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# misc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/preprocessed_train_values.csv')\n",
    "X_check = pd.read_csv('data/preprocessed_test_values.csv')\n",
    "y = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "del X['Unnamed: 0']\n",
    "del X_check['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>plan_configuration_n</th>\n",
       "      <th>plan_configuration_o</th>\n",
       "      <th>plan_configuration_q</th>\n",
       "      <th>plan_configuration_s</th>\n",
       "      <th>plan_configuration_u</th>\n",
       "      <th>legal_ownership_status_a</th>\n",
       "      <th>legal_ownership_status_r</th>\n",
       "      <th>legal_ownership_status_v</th>\n",
       "      <th>legal_ownership_status_w</th>\n",
       "      <th>building_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>12198</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2812</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>363</td>\n",
       "      <td>8973</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>418</td>\n",
       "      <td>10694</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>590882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>1488</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   geo_level_1_id  geo_level_2_id  geo_level_3_id  count_floors_pre_eq  age  \\\n",
       "0               6             487           12198                    2   30   \n",
       "1               8             900            2812                    2   10   \n",
       "2              21             363            8973                    2   10   \n",
       "3              22             418           10694                    2   10   \n",
       "4              11             131            1488                    3   30   \n",
       "\n",
       "   area_percentage  height_percentage  has_superstructure_adobe_mud  \\\n",
       "0                6                  5                             1   \n",
       "1                8                  7                             0   \n",
       "2                5                  5                             0   \n",
       "3                6                  5                             0   \n",
       "4                8                  9                             1   \n",
       "\n",
       "   has_superstructure_mud_mortar_stone  has_superstructure_stone_flag  ...  \\\n",
       "0                                    1                              0  ...   \n",
       "1                                    1                              0  ...   \n",
       "2                                    1                              0  ...   \n",
       "3                                    1                              0  ...   \n",
       "4                                    0                              0  ...   \n",
       "\n",
       "   plan_configuration_n  plan_configuration_o  plan_configuration_q  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   plan_configuration_s  plan_configuration_u  legal_ownership_status_a  \\\n",
       "0                   0.0                   0.0                       0.0   \n",
       "1                   0.0                   0.0                       0.0   \n",
       "2                   0.0                   0.0                       0.0   \n",
       "3                   0.0                   0.0                       0.0   \n",
       "4                   0.0                   0.0                       0.0   \n",
       "\n",
       "   legal_ownership_status_r  legal_ownership_status_v  \\\n",
       "0                       0.0                       1.0   \n",
       "1                       0.0                       1.0   \n",
       "2                       0.0                       1.0   \n",
       "3                       0.0                       1.0   \n",
       "4                       0.0                       1.0   \n",
       "\n",
       "   legal_ownership_status_w  building_id  \n",
       "0                       0.0       802906  \n",
       "1                       0.0        28830  \n",
       "2                       0.0        94947  \n",
       "3                       0.0       590882  \n",
       "4                       0.0       201944  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = X.columns[:7]\n",
    "mask = []\n",
    "for cols in columns_to_scale:\n",
    "    mask.append(cols)\n",
    "\n",
    "enc = StandardScaler()\n",
    "\n",
    "X_to_scale = X[mask]\n",
    "\n",
    "X.drop(mask,axis = 1)\n",
    "\n",
    "X_encoded = enc.fit_transform(X_to_scale)\n",
    "\n",
    "X[columns_to_scale] = X_encoded\n",
    "\n",
    "\n",
    "del X_to_scale\n",
    "del X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = X_check.columns[:7]\n",
    "mask = []\n",
    "for cols in columns_to_scale:\n",
    "    mask.append(cols)\n",
    "\n",
    "enc = StandardScaler()\n",
    "\n",
    "X_check_to_scale = X_check[mask]\n",
    "\n",
    "X_check.drop(mask,axis = 1)\n",
    "\n",
    "X_encoded = enc.fit_transform(X_check_to_scale)\n",
    "\n",
    "X_check[columns_to_scale] = X_encoded\n",
    "\n",
    "\n",
    "del X_check_to_scale\n",
    "del X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempts of feature engineering\n",
    "\n",
    "The idea is to cluster the values by their geographical proximity, as it is likely that the level of damage dealt is quite equal in a geographical region due to similar buildings quality. As first three features are connected with the question, but obviously have different weights as impacts on the geographical placing, we will assign naive weights of e2,e1,1 accordingly and use KMeans to figure out at least the number of clusters. Before we found the amount of sensible clusters as 17 using Naive DBSCAN on test_data with eps = 35.\n",
    "\n",
    "\n",
    "The epsilon hyperparameter was chosen thus to create sensible finite number of clusters. Clustering should be stable on all datasets to be considered as sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def create_clustering_data(dataset): #instance of np.ndarray\n",
    "    subdata = dataset[:,0:3].copy()\n",
    "    return subdata\n",
    "\n",
    "def perform_clustering(subdata,col_weights = [100,10,1]):\n",
    "    for i in range(len(col_weights)):\n",
    "        subdata[:,i] *= col_weights[i]\n",
    "        \n",
    "    print(subdata.shape)\n",
    "    #print('...clustering in process...')\n",
    "    clustering = KMeans(17).fit(subdata)\n",
    "    print('...clustering done.')\n",
    "    #print(clustering.labels_.shape)\n",
    "    \n",
    "    return clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8', 'cluster_9', 'cluster_10', 'cluster_11', 'cluster_12', 'cluster_13', 'cluster_14', 'cluster_15', 'cluster_16']\n"
     ]
    }
   ],
   "source": [
    "enc_cols = ['cluster_' + str(i) for i in range(17)]\n",
    "print(enc_cols)\n",
    "\n",
    "def append_clusters_to_dataset(main_set, clustering):\n",
    "    enc = OneHotEncoder()\n",
    "    ohe = enc.fit_transform(clustering.reshape(-1,1))\n",
    "    ohe = ohe.toarray()\n",
    "    add_on = pd.DataFrame(ohe,columns = enc_cols)\n",
    "    \n",
    "    X = pd.concat([main_set,add_on],axis = 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260601, 3)\n",
      "...clustering done.\n"
     ]
    }
   ],
   "source": [
    "sub_X = create_clustering_data(np.array(X))\n",
    "clustering_main = perform_clustering(sub_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id',\n",
       "       'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
       "       'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
       "       'has_superstructure_stone_flag',\n",
       "       'has_superstructure_cement_mortar_stone',\n",
       "       'has_superstructure_mud_mortar_brick',\n",
       "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
       "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
       "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
       "       'count_families', 'has_secondary_use', 'has_secondary_use_agriculture',\n",
       "       'has_secondary_use_hotel', 'has_secondary_use_rental',\n",
       "       'has_secondary_use_institution', 'has_secondary_use_school',\n",
       "       'has_secondary_use_industry', 'has_secondary_use_health_post',\n",
       "       'has_secondary_use_gov_office', 'has_secondary_use_use_police',\n",
       "       'has_secondary_use_other', 'land_surface_condition_n',\n",
       "       'land_surface_condition_o', 'land_surface_condition_t',\n",
       "       'foundation_type_h', 'foundation_type_i', 'foundation_type_r',\n",
       "       'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_q',\n",
       "       'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_m',\n",
       "       'ground_floor_type_v', 'ground_floor_type_x', 'ground_floor_type_z',\n",
       "       'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s',\n",
       "       'other_floor_type_x', 'position_j', 'position_o', 'position_s',\n",
       "       'position_t', 'plan_configuration_a', 'plan_configuration_c',\n",
       "       'plan_configuration_d', 'plan_configuration_f', 'plan_configuration_m',\n",
       "       'plan_configuration_n', 'plan_configuration_o', 'plan_configuration_q',\n",
       "       'plan_configuration_s', 'plan_configuration_u',\n",
       "       'legal_ownership_status_a', 'legal_ownership_status_r',\n",
       "       'legal_ownership_status_v', 'legal_ownership_status_w', 'building_id',\n",
       "       'cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'cluster_4',\n",
       "       'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8', 'cluster_9',\n",
       "       'cluster_10', 'cluster_11', 'cluster_12', 'cluster_13', 'cluster_14',\n",
       "       'cluster_15', 'cluster_16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = append_clusters_to_dataset(X, clustering_main)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86868, 3)\n",
      "...clustering done.\n"
     ]
    }
   ],
   "source": [
    "sub_X_check = create_clustering_data(np.array(X_check))\n",
    "clustering_check = perform_clustering(sub_X_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id',\n",
       "       'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
       "       'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
       "       'has_superstructure_stone_flag',\n",
       "       'has_superstructure_cement_mortar_stone',\n",
       "       'has_superstructure_mud_mortar_brick',\n",
       "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
       "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
       "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
       "       'count_families', 'has_secondary_use', 'has_secondary_use_agriculture',\n",
       "       'has_secondary_use_hotel', 'has_secondary_use_rental',\n",
       "       'has_secondary_use_institution', 'has_secondary_use_school',\n",
       "       'has_secondary_use_industry', 'has_secondary_use_health_post',\n",
       "       'has_secondary_use_gov_office', 'has_secondary_use_use_police',\n",
       "       'has_secondary_use_other', 'land_surface_condition_n',\n",
       "       'land_surface_condition_o', 'land_surface_condition_t',\n",
       "       'foundation_type_h', 'foundation_type_i', 'foundation_type_r',\n",
       "       'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_q',\n",
       "       'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_m',\n",
       "       'ground_floor_type_v', 'ground_floor_type_x', 'ground_floor_type_z',\n",
       "       'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s',\n",
       "       'other_floor_type_x', 'position_j', 'position_o', 'position_s',\n",
       "       'position_t', 'plan_configuration_a', 'plan_configuration_c',\n",
       "       'plan_configuration_d', 'plan_configuration_f', 'plan_configuration_m',\n",
       "       'plan_configuration_n', 'plan_configuration_o', 'plan_configuration_q',\n",
       "       'plan_configuration_s', 'plan_configuration_u',\n",
       "       'legal_ownership_status_a', 'legal_ownership_status_r',\n",
       "       'legal_ownership_status_v', 'legal_ownership_status_w', 'building_id',\n",
       "       'cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'cluster_4',\n",
       "       'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8', 'cluster_9',\n",
       "       'cluster_10', 'cluster_11', 'cluster_12', 'cluster_13', 'cluster_14',\n",
       "       'cluster_15', 'cluster_16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_check = append_clusters_to_dataset(X_check, clustering_main)\n",
    "X_check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X.sort_values('building_id'),\n",
    "                                                 y.sort_values('building_id')['damage_grade'],\n",
    "                                                 test_size = 0.2,\n",
    "                                                 random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "y_test = np.array(y_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=3)]: Done  35 out of  35 | elapsed:   25.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(), n_jobs=3,\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04])},\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = LogisticRegression(penalty='l2')\n",
    "\n",
    "param_grid = {'C' : np.logspace(-2,4,7)}\n",
    "\n",
    "log_grid = GridSearchCV(model,param_grid=param_grid,scoring = 'accuracy',verbose=10,n_jobs=3)\n",
    "\n",
    "log_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5688877803572456\n"
     ]
    }
   ],
   "source": [
    "y_logreg_pred = log_grid.predict(X_test)\n",
    "\n",
    "print(f1_score(y_logreg_pred,y_test,average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- / fitting / ---\n",
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed: 34.7min\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed: 58.9min\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed: 76.7min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed: 95.9min\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed: 122.0min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed: 150.4min\n",
      "[Parallel(n_jobs=3)]: Done 175 out of 175 | elapsed: 185.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:33:01] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { num_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=3,\n",
       "             param_grid={'max_depth': range(2, 16, 2),\n",
       "                         'num_estimators': [10, 20, 50, 100, 200]},\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "\n",
    "param_grid = {'max_depth' : range(2,16,2),\n",
    "             'num_estimators' : [10,20,50,100,200]}\n",
    "\n",
    "boost_grid = GridSearchCV(model,param_grid=param_grid,scoring = 'accuracy',verbose=10,n_jobs=3)\n",
    "\n",
    "print('--- / fitting / ---')\n",
    "\n",
    "boost_grid.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SCORE:\n",
      "0.7360181117016174\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "y_boost_pred = boost_grid.predict(X_test)\n",
    "\n",
    "print('='*100)\n",
    "print('SCORE:')\n",
    "print(f1_score(y_test,y_boost_pred,average='micro'))\n",
    "\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model was (100,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SCORE:\n",
      "0.7364402064427006\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "### Additional re-check of non-val score with features added. Slight increase.\n",
    "\n",
    "best_model = XGBClassifier(n_estimators = 100,max_depth = 12)\n",
    "\n",
    "best_model.fit(X_train,y_train)\n",
    "\n",
    "y_boost = best_model.predict(X_test)\n",
    "\n",
    "print('='*100)\n",
    "print('SCORE:')\n",
    "print(f1_score(y_test,y_boost,average='micro'))\n",
    "\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = best_model.fit(X_train,y_train).feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208480, 86)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geo_level_1_id', 'has_superstructure_mud_mortar_stone',\n",
       "       'has_superstructure_cement_mortar_brick', 'foundation_type_i',\n",
       "       'foundation_type_r', 'ground_floor_type_v'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe # so there are 69 starting columns + 17 clusters in the end. We see that some clusters bring in tremendous impact.\n",
    "\n",
    "X.columns[fe[:69]>0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cluster_4', 'cluster_11', 'cluster_12', 'cluster_14',\n",
       "       'cluster_15', 'cluster_16'], dtype='<U10')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(enc_cols)[fe[69:] > 0.01] #important clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refitting model on the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estim = XGBClassifier(n_estimators = 100,max_depth = 12)\n",
    "\n",
    "estim.fit(X[:,3:],y['damage_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on the X_check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_pred = estim.predict(X_check[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868,)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = X_check['building_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_all_pred,building_ids):\n",
    "\n",
    "    building_ids = np.array(X_check['building_id'])\n",
    "    preds = y_all_pred\n",
    "    sub = pd.DataFrame()\n",
    "    sub['building_id'] = building_ids\n",
    "    sub['damage_grade'] = y_all_pred\n",
    "    sub.set_index('building_id',inplace = True)\n",
    "    print(sub.shape)\n",
    "    sub.to_csv('submissions/sub3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86868, 1)\n"
     ]
    }
   ],
   "source": [
    "create_submission(y_all_pred,building_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
