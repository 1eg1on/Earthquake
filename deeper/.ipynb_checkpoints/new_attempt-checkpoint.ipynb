{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ha-ha classics\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import os \n",
    "\n",
    "# sklearn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "# 2nd category\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier as lgbm\n",
    "\n",
    "# 3rd category\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# misc\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_values (1).csv')\n",
    "labels = pd.read_csv('data/train_labels (1).csv')\n",
    "test = pd.read_csv('data/test_values (1).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ids = train.columns[0]\n",
    "geo_levels = train.columns[1:4]\n",
    "numeric = list(train.columns[4:8])\n",
    "categorical = list(train.columns[8:15])\n",
    "categorical.append(train.columns[26])\n",
    "flags = train.columns[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type', 'other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status']\n"
     ]
    }
   ],
   "source": [
    "b_ids = train.columns[0]\n",
    "geo_levels = train.columns[1:4]\n",
    "numeric = train.columns[4:8]\n",
    "categorical = list(train.columns[8:15])\n",
    "categorical.append(train.columns[26])\n",
    "print(categorical)\n",
    "flags = train.columns[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(data,column,labels):\n",
    "    mapping = {}\n",
    "    \n",
    "    for value in data[column].unique():\n",
    "        #print(data[column])\n",
    "        mask = np.array(data[column] == value)\n",
    "        print('Value encoding: {}, \\t mask shape: {}'.format(value, mask.shape))\n",
    "        labels_mean = (labels['damage_grade'][mask]).mean()\n",
    "        mapping[value] = labels_mean\n",
    "    print('mapping constructed.')\n",
    "    #print(mapping)\n",
    "    print('processing data...')\n",
    "    data[column] = data[column].apply(lambda x: mapping[x])\n",
    "    \n",
    "    return mapping,data\n",
    "    \n",
    "def target_encode_test(data_test,column,mapping):\n",
    "    data_test[column] = data_test[column].apply(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value encoding: t, \t mask shape: (260601,)\n",
      "Value encoding: o, \t mask shape: (260601,)\n",
      "Value encoding: n, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n",
      "Value encoding: r, \t mask shape: (260601,)\n",
      "Value encoding: w, \t mask shape: (260601,)\n",
      "Value encoding: i, \t mask shape: (260601,)\n",
      "Value encoding: u, \t mask shape: (260601,)\n",
      "Value encoding: h, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n",
      "Value encoding: n, \t mask shape: (260601,)\n",
      "Value encoding: q, \t mask shape: (260601,)\n",
      "Value encoding: x, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n",
      "Value encoding: f, \t mask shape: (260601,)\n",
      "Value encoding: x, \t mask shape: (260601,)\n",
      "Value encoding: v, \t mask shape: (260601,)\n",
      "Value encoding: z, \t mask shape: (260601,)\n",
      "Value encoding: m, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n",
      "Value encoding: q, \t mask shape: (260601,)\n",
      "Value encoding: x, \t mask shape: (260601,)\n",
      "Value encoding: j, \t mask shape: (260601,)\n",
      "Value encoding: s, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n",
      "Value encoding: t, \t mask shape: (260601,)\n",
      "Value encoding: s, \t mask shape: (260601,)\n",
      "Value encoding: j, \t mask shape: (260601,)\n",
      "Value encoding: o, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n",
      "Value encoding: d, \t mask shape: (260601,)\n",
      "Value encoding: u, \t mask shape: (260601,)\n",
      "Value encoding: s, \t mask shape: (260601,)\n",
      "Value encoding: q, \t mask shape: (260601,)\n",
      "Value encoding: m, \t mask shape: (260601,)\n",
      "Value encoding: c, \t mask shape: (260601,)\n",
      "Value encoding: a, \t mask shape: (260601,)\n",
      "Value encoding: n, \t mask shape: (260601,)\n",
      "Value encoding: f, \t mask shape: (260601,)\n",
      "Value encoding: o, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n",
      "Value encoding: v, \t mask shape: (260601,)\n",
      "Value encoding: a, \t mask shape: (260601,)\n",
      "Value encoding: r, \t mask shape: (260601,)\n",
      "Value encoding: w, \t mask shape: (260601,)\n",
      "mapping constructed.\n",
      "processing data...\n"
     ]
    }
   ],
   "source": [
    "for columns in categorical:\n",
    "    mapping,train = target_encode(train,columns,labels)\n",
    "    #print(train[for_target])\n",
    "    target_encode_test(test,columns,mapping)\n",
    "    #print(test[for_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe(dataset):\n",
    "    '''\n",
    "    dataset - supposed to be train or test pd.DataFrames\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dataset['height_area_ratio'] = dataset['height_percentage']/dataset['area_percentage']\n",
    "    dataset['is_old'] = dataset['age']>20\n",
    "    dataset['age_sq'] = dataset['age']**2\n",
    "    \n",
    "fe(train)\n",
    "fe(test)\n",
    "\n",
    "numeric.extend(['height_area_ratio','is_old','age_sq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering for geolevels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8', 'cluster_9', 'cluster_10', 'cluster_11', 'cluster_12', 'cluster_13', 'cluster_14', 'cluster_15', 'cluster_16', 'cluster_17', 'cluster_18', 'cluster_19']\n"
     ]
    }
   ],
   "source": [
    "enc_cols = ['cluster_' + str(i) for i in range(20)]\n",
    "print(enc_cols)\n",
    "\n",
    "def append_clusters_to_dataset(main_set, clustering):\n",
    "    enc = OneHotEncoder()\n",
    "    ohe = enc.fit_transform(clustering.reshape(-1,1))\n",
    "    ohe = ohe.toarray()\n",
    "    add_on = pd.DataFrame(ohe,columns = enc_cols)\n",
    "    \n",
    "    X = pd.concat([main_set,add_on],axis = 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def create_clustering_data(dataset): #instance of np.ndarray\n",
    "    subdata = dataset[:,1:4].copy()\n",
    "    return subdata\n",
    "\n",
    "def perform_clustering(subdata,col_weights = [100,10,1]):\n",
    "    for i in range(len(col_weights)):\n",
    "        subdata[:,i] *= col_weights[i]\n",
    "        \n",
    "    #print(subdata.shape)\n",
    "    print('...clustering in process...')\n",
    "    clustering = KMeans(20).fit(subdata)\n",
    "    print('...clustering done.')\n",
    "    #print(clustering.labels_.shape)\n",
    "    \n",
    "    return clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsub_train = create_clustering_data(np.array(train))\\nclustering_train = perform_clustering(sub_train)\\n\\ntrain = append_clusters_to_dataset(train, clustering_train)\\n\\n#### Processing check\\n\\nsub_test = create_clustering_data(np.array(test))\\nclustering_test = perform_clustering(sub_test)\\n\\ntest = append_clusters_to_dataset(test, clustering_test)\\nprint(test.shape)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sub_train = create_clustering_data(np.array(train))\n",
    "clustering_train = perform_clustering(sub_train)\n",
    "\n",
    "train = append_clusters_to_dataset(train, clustering_train)\n",
    "\n",
    "#### Processing check\n",
    "\n",
    "sub_test = create_clustering_data(np.array(test))\n",
    "clustering_test = perform_clustering(sub_test)\n",
    "\n",
    "test = append_clusters_to_dataset(test, clustering_test)\n",
    "print(test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandartScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = StandardScaler()\n",
    "\n",
    "train[numeric] = enc.fit_transform(train[numeric])\n",
    "test[numeric] = enc.fit_transform(test[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def ohe(data,categorical):\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    one_hot = enc.fit_transform(data[categorical]).toarray()\n",
    "    #print(one_hot.shape)\n",
    "    \n",
    "    cols = categorical\n",
    "    #print(cols)\n",
    "    \n",
    "    cats = enc.categories_\n",
    "    #print(cats[0])\n",
    "\n",
    "    new_column_names = []\n",
    "    for k in range(len(cols)):\n",
    "        for levels in range(len(cats[k])):\n",
    "            #print()\n",
    "            new_column_names.append(cols[k] + '_' + cats[k][levels])\n",
    "            \n",
    "    #print(new_column_names)     \n",
    "    \n",
    "    categorical_dataframe = pd.DataFrame(one_hot, columns = new_column_names)\n",
    "    new_dataset = pd.concat([data.drop(categorical,axis = 1),categorical_dataframe],axis = 1)\n",
    "    return new_dataset\n",
    "\n",
    "train = ohe(train,categorical)\n",
    "test = ohe(test,categorical)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_geo(dataset):\n",
    "    for cols in dataset.columns:\n",
    "        if 'geo' in cols:\n",
    "            dataset.drop(cols,inplace = True,axis = 1)\n",
    "#drop_geo(train)\n",
    "#drop_geo(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = test['building_id'].copy()\n",
    "train.drop('building_id',axis = 1,inplace = True)\n",
    "test.drop('building_id',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labels['damage_grade']).ravel()\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(train,\n",
    "                                                 y,\n",
    "                                                 random_state = 1003,\n",
    "                                                test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.597118\n",
      "[100]\tvalid_0's multi_logloss: 0.562885\n",
      "[150]\tvalid_0's multi_logloss: 0.539178\n",
      "[200]\tvalid_0's multi_logloss: 0.521119\n",
      "[250]\tvalid_0's multi_logloss: 0.504924\n",
      "[300]\tvalid_0's multi_logloss: 0.490071\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[298]\tvalid_0's multi_logloss: 0.486654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(booster='gbtree', learning_rate=0.6, n_estimators=300, n_jobs=4,\n",
       "               objective='multi_logloss', subsample=0.7, verbosity=1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = lgbm.LGBMClassifier(n_estimators = 100,\n",
    "                    objective = 'multi_logloss',\n",
    "                    learning_rate = 0.6,\n",
    "                    verbosity = 1,\n",
    "                    booster = 'gbtree',\n",
    "                    n_jobs = 4,\n",
    "                    subsample = 0.6\n",
    "                    )\n",
    "\n",
    "xgb.fit(train,\n",
    "        y,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose = 50,\n",
    "        eval_set = [(X_test, y_test)],\n",
    "        eval_metric = \"multi_logloss\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.7928322013737001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('f1: {}'.format(f1_score(y_pred,y_test,average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_pred = xgb.predict(np.array(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r = pd.read_csv('data/test_values (1).csv')\n",
    "building_ids = test_r['building_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_all_pred,building_ids,name):\n",
    "\n",
    "    preds = y_all_pred\n",
    "    sub = pd.DataFrame()\n",
    "    sub['building_id'] = building_ids\n",
    "    sub['damage_grade'] = y_all_pred\n",
    "    sub.set_index('building_id',inplace = True)\n",
    "    print(sub.shape)\n",
    "    sub.to_csv('subs/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86868, 1)\n"
     ]
    }
   ],
   "source": [
    "name = 'sub_xgb_mix_model3'\n",
    "create_submission(y_all_pred,building_ids,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = pd.read_csv('subs/sub_xgb_mix_model1.csv')\n",
    "m2 = pd.read_csv('subs/sub_xgb_mix_model2.csv')\n",
    "m3 = pd.read_csv('subs/sub_xgb_mix_model3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = m1['damage_grade']\n",
    "pred2 = m2['damage_grade']\n",
    "pred3 = m3['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.median([pred1,pred2,pred3],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9575217571487774"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output == pred3).sum()/len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_pred = output.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r = pd.read_csv('data/test_values (1).csv')\n",
    "building_ids = test_r['building_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_all_pred,building_ids,name):\n",
    "\n",
    "    preds = y_all_pred\n",
    "    sub = pd.DataFrame()\n",
    "    sub['building_id'] = building_ids\n",
    "    sub['damage_grade'] = y_all_pred\n",
    "    sub.set_index('building_id',inplace = True)\n",
    "    print(sub.shape)\n",
    "    sub.to_csv('subs/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86868, 1)\n"
     ]
    }
   ],
   "source": [
    "name = 'sub_xgb_mix_output'\n",
    "create_submission(y_all_pred,building_ids,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.83093\n",
      "Will train until validation_0-mlogloss hasn't improved in 20 rounds.\n",
      "[10]\tvalidation_0-mlogloss:0.47177\n",
      "[20]\tvalidation_0-mlogloss:0.41560\n",
      "[30]\tvalidation_0-mlogloss:0.36628\n",
      "[40]\tvalidation_0-mlogloss:0.32601\n",
      "[50]\tvalidation_0-mlogloss:0.29168\n",
      "[60]\tvalidation_0-mlogloss:0.26412\n",
      "[64]\tvalidation_0-mlogloss:0.25316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▊                                                                       | 1/7 [08:38<51:52, 518.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.02055\n",
      "Will train until validation_0-mlogloss hasn't improved in 20 rounds.\n",
      "[10]\tvalidation_0-mlogloss:0.65894\n",
      "[20]\tvalidation_0-mlogloss:0.55627\n",
      "[30]\tvalidation_0-mlogloss:0.50941\n",
      "[40]\tvalidation_0-mlogloss:0.48128\n",
      "[47]\tvalidation_0-mlogloss:0.46634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████▋                                                           | 2/7 [16:29<42:01, 504.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.92930\n",
      "Will train until validation_0-mlogloss hasn't improved in 20 rounds.\n",
      "[10]\tvalidation_0-mlogloss:0.53154\n",
      "[20]\tvalidation_0-mlogloss:0.46494\n",
      "[30]\tvalidation_0-mlogloss:0.42692\n",
      "[40]\tvalidation_0-mlogloss:0.39867\n",
      "[50]\tvalidation_0-mlogloss:0.37410\n",
      "[60]\tvalidation_0-mlogloss:0.34997\n",
      "[70]\tvalidation_0-mlogloss:0.33047\n",
      "[74]\tvalidation_0-mlogloss:0.32303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▌                                               | 3/7 [27:32<36:47, 551.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.84143\n",
      "Will train until validation_0-mlogloss hasn't improved in 20 rounds.\n",
      "[10]\tvalidation_0-mlogloss:0.48649\n",
      "[20]\tvalidation_0-mlogloss:0.42973\n",
      "[30]\tvalidation_0-mlogloss:0.38621\n",
      "[40]\tvalidation_0-mlogloss:0.35118\n",
      "[50]\tvalidation_0-mlogloss:0.31834\n",
      "[60]\tvalidation_0-mlogloss:0.28871\n",
      "[70]\tvalidation_0-mlogloss:0.26205\n",
      "[75]\tvalidation_0-mlogloss:0.25082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████▍                                   | 4/7 [39:09<29:46, 595.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.08604\n",
      "Will train until validation_0-mlogloss hasn't improved in 20 rounds.\n",
      "[10]\tvalidation_0-mlogloss:0.95605\n",
      "[20]\tvalidation_0-mlogloss:0.86052\n",
      "[30]\tvalidation_0-mlogloss:0.78763\n",
      "[40]\tvalidation_0-mlogloss:0.73366\n",
      "[46]\tvalidation_0-mlogloss:0.70735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████████████████████▎                       | 5/7 [42:22<15:49, 474.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.83728\n",
      "Will train until validation_0-mlogloss hasn't improved in 20 rounds.\n",
      "[10]\tvalidation_0-mlogloss:0.50402\n",
      "[20]\tvalidation_0-mlogloss:0.43541\n",
      "[30]\tvalidation_0-mlogloss:0.39514\n",
      "[40]\tvalidation_0-mlogloss:0.36230\n",
      "[50]\tvalidation_0-mlogloss:0.33095\n",
      "[60]\tvalidation_0-mlogloss:0.30147\n",
      "[70]\tvalidation_0-mlogloss:0.27669\n",
      "[71]\tvalidation_0-mlogloss:0.27465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████▏           | 6/7 [44:51<06:16, 376.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.78076\n",
      "Will train until validation_0-mlogloss hasn't improved in 20 rounds.\n",
      "[10]\tvalidation_0-mlogloss:0.46131\n",
      "[20]\tvalidation_0-mlogloss:0.39750\n",
      "[30]\tvalidation_0-mlogloss:0.35056\n",
      "[40]\tvalidation_0-mlogloss:0.30610\n",
      "[50]\tvalidation_0-mlogloss:0.27014\n",
      "[60]\tvalidation_0-mlogloss:0.24173\n",
      "[70]\tvalidation_0-mlogloss:0.21568\n",
      "[76]\tvalidation_0-mlogloss:0.20239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [47:37<00:00, 408.20s/it]\n"
     ]
    }
   ],
   "source": [
    "model_outputs = {}\n",
    "\n",
    "n = 7\n",
    "model_seeds = np.random.randint(1,10000,n)\n",
    "n_estimatorss = np.random.randint(40,80,n)\n",
    "learning_rates = np.random.uniform(0,1,n)\n",
    "\n",
    "for iters in tqdm(range(n)):\n",
    "    \n",
    "    xgb = XGBClassifier(n_estimators = n_estimatorss[iters], #\n",
    "                    objective = 'mse',\n",
    "                    learning_rate = learning_rates[iters],\n",
    "                    max_depth = 13,\n",
    "                    colsample_bytree=0.75,\n",
    "                    verbosity = 1,\n",
    "                    booster = 'gbtree',\n",
    "                    n_jobs = 4,\n",
    "                    subsample = 0.75,\n",
    "                    random_state = model_seeds[iters],\n",
    "                    \n",
    "                    )\n",
    "\n",
    "    xgb.fit(train,\n",
    "            y,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose = 10,\n",
    "            eval_set = [(train,y)],\n",
    "            eval_metric = \"mlogloss\"\n",
    "           )\n",
    "    \n",
    "    y_all_pred = xgb.predict(np.array(test))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_outputs[iters] = y_all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868,)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = np.median(np.concatenate(list(model_outputs.values())).reshape(n,-1),axis = 0)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_pred = output.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f1_score(y_all_pred,y_test,average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r =pd.read_csv('data/test_values (1).csv')\n",
    "building_ids = test_r['building_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_all_pred,building_ids,name):\n",
    "\n",
    "    preds = y_all_pred\n",
    "    sub = pd.DataFrame()\n",
    "    sub['building_id'] = building_ids\n",
    "    sub['damage_grade'] = y_all_pred\n",
    "    sub.set_index('building_id',inplace = True)\n",
    "    print(sub.shape)\n",
    "    sub.to_csv('subs/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86868, 1)\n"
     ]
    }
   ],
   "source": [
    "name = '8_xgb'\n",
    "create_submission(y_all_pred,building_ids,name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
