{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ha-ha classics\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import os \n",
    "\n",
    "# sklearn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "# 2nd category\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier as lgbm\n",
    "\n",
    "# 3rd category\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# misc\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_values (1).csv')\n",
    "labels = pd.read_csv('data/train_labels (1).csv')\n",
    "test = pd.read_csv('data/test_values (1).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ids = train.columns[0]\n",
    "geo_levels = train.columns[1:4]\n",
    "numeric = train.columns[4:8]\n",
    "categorical = list(train.columns[8:15])\n",
    "categorical.append(train.columns[26])\n",
    "flags = train.columns[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe(dataset):\n",
    "    '''\n",
    "    dataset - supposed to be train or test pd.DataFrames\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dataset['height_area_ratio'] = dataset['height_percentage']/dataset['area_percentage']\n",
    "    dataset['is_old'] = dataset['age']>10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering for geolevels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8', 'cluster_9', 'cluster_10', 'cluster_11', 'cluster_12', 'cluster_13', 'cluster_14', 'cluster_15', 'cluster_16']\n"
     ]
    }
   ],
   "source": [
    "enc_cols = ['cluster_' + str(i) for i in range(17)]\n",
    "print(enc_cols)\n",
    "\n",
    "def append_clusters_to_dataset(main_set, clustering):\n",
    "    enc = OneHotEncoder()\n",
    "    ohe = enc.fit_transform(clustering.reshape(-1,1))\n",
    "    ohe = ohe.toarray()\n",
    "    add_on = pd.DataFrame(ohe,columns = enc_cols)\n",
    "    \n",
    "    X = pd.concat([main_set,add_on],axis = 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def create_clustering_data(dataset): #instance of np.ndarray\n",
    "    subdata = dataset[:,1:4].copy()\n",
    "    return subdata\n",
    "\n",
    "def perform_clustering(subdata,col_weights = [100,10,1]):\n",
    "    for i in range(len(col_weights)):\n",
    "        subdata[:,i] *= col_weights[i]\n",
    "        \n",
    "    #print(subdata.shape)\n",
    "    print('...clustering in process...')\n",
    "    clustering = KMeans(17).fit(subdata)\n",
    "    print('...clustering done.')\n",
    "    #print(clustering.labels_.shape)\n",
    "    \n",
    "    return clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...clustering in process...\n",
      "...clustering done.\n"
     ]
    }
   ],
   "source": [
    "sub_train = create_clustering_data(np.array(train))\n",
    "clustering_train = perform_clustering(sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = append_clusters_to_dataset(train, clustering_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...clustering in process...\n",
      "...clustering done.\n"
     ]
    }
   ],
   "source": [
    "sub_test = create_clustering_data(np.array(test))\n",
    "clustering_test = perform_clustering(sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86868, 56)\n"
     ]
    }
   ],
   "source": [
    "test = append_clusters_to_dataset(test, clustering_test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandartScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = StandardScaler()\n",
    "\n",
    "train[numeric] = enc.fit_transform(train[numeric])\n",
    "test[numeric] = enc.fit_transform(test[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(data,categorical):\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    one_hot = enc.fit_transform(data[categorical]).toarray()\n",
    "    #print(one_hot.shape)\n",
    "    \n",
    "    cols = categorical\n",
    "    #print(cols)\n",
    "    \n",
    "    cats = enc.categories_\n",
    "    #print(cats[0])\n",
    "\n",
    "    new_column_names = []\n",
    "    for k in range(len(cols)):\n",
    "        for levels in range(len(cats[k])):\n",
    "            #print()\n",
    "            new_column_names.append(cols[k] + '_' + cats[k][levels])\n",
    "            \n",
    "    #print(new_column_names)     \n",
    "    \n",
    "    categorical_dataframe = pd.DataFrame(one_hot, columns = new_column_names)\n",
    "    new_dataset = pd.concat([data.drop(categorical,axis = 1),categorical_dataframe],axis = 1)\n",
    "    return new_dataset\n",
    "\n",
    "train = ohe(train,categorical)\n",
    "test = ohe(test,categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_geo(dataset):\n",
    "    for cols in dataset.columns:\n",
    "        if 'geo' in cols:\n",
    "            dataset.drop(cols,inplace = True,axis = 1)\n",
    "#drop_geo(train)\n",
    "#drop_geo(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = test['building_id'].copy()\n",
    "train.drop('building_id',axis = 1,inplace = True)\n",
    "test.drop('building_id',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labels['damage_grade']).ravel()\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(train,\n",
    "                                                 y,\n",
    "                                                 random_state = 1003,\n",
    "                                                test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.85956\n",
      "Will train until validation_0-mlogloss hasn't improved in 5 rounds.\n",
      "[2]\tvalidation_0-mlogloss:0.70237\n",
      "[4]\tvalidation_0-mlogloss:0.64999\n",
      "[6]\tvalidation_0-mlogloss:0.62732\n",
      "[8]\tvalidation_0-mlogloss:0.61442\n",
      "[10]\tvalidation_0-mlogloss:0.60741\n",
      "[12]\tvalidation_0-mlogloss:0.60068\n",
      "[14]\tvalidation_0-mlogloss:0.59671\n",
      "[16]\tvalidation_0-mlogloss:0.59400\n",
      "[18]\tvalidation_0-mlogloss:0.59093\n",
      "[20]\tvalidation_0-mlogloss:0.58808\n",
      "[22]\tvalidation_0-mlogloss:0.58634\n",
      "[24]\tvalidation_0-mlogloss:0.58464\n",
      "[26]\tvalidation_0-mlogloss:0.58233\n",
      "[28]\tvalidation_0-mlogloss:0.58114\n",
      "[30]\tvalidation_0-mlogloss:0.57956\n",
      "[32]\tvalidation_0-mlogloss:0.57824\n",
      "[34]\tvalidation_0-mlogloss:0.57725\n",
      "[36]\tvalidation_0-mlogloss:0.57669\n",
      "[38]\tvalidation_0-mlogloss:0.57565\n",
      "[40]\tvalidation_0-mlogloss:0.57510\n",
      "[42]\tvalidation_0-mlogloss:0.57469\n",
      "[44]\tvalidation_0-mlogloss:0.57421\n",
      "[46]\tvalidation_0-mlogloss:0.57381\n",
      "[48]\tvalidation_0-mlogloss:0.57390\n",
      "[50]\tvalidation_0-mlogloss:0.57366\n",
      "[52]\tvalidation_0-mlogloss:0.57337\n",
      "[54]\tvalidation_0-mlogloss:0.57342\n",
      "[56]\tvalidation_0-mlogloss:0.57351\n",
      "[58]\tvalidation_0-mlogloss:0.57370\n",
      "Stopping. Best iteration:\n",
      "[53]\tvalidation_0-mlogloss:0.57336\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.5, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=4,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.9,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators = 100,\n",
    "                    max_depth = 12,\n",
    "                    learning_rate = 0.5,\n",
    "                    verbosity = 1,\n",
    "                    booster = 'gbtree',\n",
    "                    n_jobs = 4,\n",
    "                    subsample = 0.9,\n",
    "                    num_parallel_tree=4\n",
    "                    )\n",
    "\n",
    "xgb.fit(X_train,\n",
    "        y_train,\n",
    "        early_stopping_rounds=5,\n",
    "        verbose = 2,\n",
    "        eval_set = [(X_test, y_test)],\n",
    "        eval_metric = \"mlogloss\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.74522745150707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('f1: {}'.format(f1_score(y_pred,y_test,average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_pred = xgb.predict(np.array(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r = pd.read_csv('data/test_values (1).csv')\n",
    "building_ids = test_r['building_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_all_pred,building_ids,name):\n",
    "\n",
    "    preds = y_all_pred\n",
    "    sub = pd.DataFrame()\n",
    "    sub['building_id'] = building_ids\n",
    "    sub['damage_grade'] = y_all_pred\n",
    "    sub.set_index('building_id',inplace = True)\n",
    "    print(sub.shape)\n",
    "    sub.to_csv('subs/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86868, 1)\n"
     ]
    }
   ],
   "source": [
    "name = 'sub_xgb_2'\n",
    "create_submission(y_all_pred,building_ids,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
